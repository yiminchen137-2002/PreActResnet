import os
import torch
import subprocess
import psutil


def optimize_system(gpu_ids=[3, 4, 5, 6]):
    """ç³»ç»Ÿçº§ä¼˜åŒ–è„šæœ¬"""

    print("=" * 60)
    print("ğŸš€ ç³»ç»Ÿæé€Ÿä¼˜åŒ–å¯åŠ¨")
    print(f"ğŸ¯ æŒ‡å®šGPU: {gpu_ids}")
    print("=" * 60)

    # 1. è®¾ç½®CUDAç¯å¢ƒå˜é‡ - æŒ‡å®šGPU
    print("\n1. ğŸ”§ è®¾ç½®CUDAç¯å¢ƒå˜é‡...")
    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, gpu_ids))
    os.environ['CUDA_CACHE_PATH'] = '/tmp/cuda-cache'
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    os.environ['NCCL_DEBUG'] = 'INFO'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'

    # 2. PyTorchæ€§èƒ½ä¼˜åŒ–
    print("2. âš¡ PyTorchæ€§èƒ½ä¼˜åŒ–...")
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # 3. æ£€æŸ¥æŒ‡å®šGPUçŠ¶æ€
    print("3. ğŸ” æ£€æŸ¥æŒ‡å®šGPUçŠ¶æ€...")
    if torch.cuda.is_available():
        visible_gpus = list(range(torch.cuda.device_count()))
        print(f"   âœ… å¯è§çš„GPUè®¾å¤‡: {visible_gpus}")

        # æ£€æŸ¥å®é™…å¯ç”¨çš„GPU
        actual_gpus = []
        for i in visible_gpus:
            try:
                torch.cuda.set_device(i)
                props = torch.cuda.get_device_properties(i)
                actual_physical_id = gpu_ids[i] if i < len(gpu_ids) else i
                actual_gpus.append(actual_physical_id)
                memory = props.total_memory / 1024 ** 3
                print(f"   ğŸ“Š GPU {actual_physical_id}(è™šæ‹Ÿ{i}): {props.name}")
                print(f"     å†…å­˜: {memory:.1f}GB")
                print(f"     Compute Capability: {props.major}.{props.minor}")
            except Exception as e:
                print(f"   âŒ GPU {gpu_ids[i]} ä¸å¯ç”¨: {e}")

        if len(actual_gpus) < len(gpu_ids):
            print(f"   âš ï¸  è­¦å‘Š: åªæœ‰ {len(actual_gpus)}/{len(gpu_ids)} ä¸ªGPUå¯ç”¨")

        # æ¸…ç†æ‰€æœ‰GPUç¼“å­˜
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()

        print(f"   ğŸ¯ å®é™…ä½¿ç”¨çš„GPU: {actual_gpus}")
    else:
        print("   âŒ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰")

    # 4. æ£€æŸ¥ç³»ç»Ÿèµ„æº
    print("4. ğŸ’» æ£€æŸ¥ç³»ç»Ÿèµ„æº...")
    cpu_count = psutil.cpu_count()
    memory = psutil.virtual_memory()
    print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
    print(f"   å†…å­˜: {memory.total / 1024 ** 3:.1f}GB, å¯ç”¨: {memory.available / 1024 ** 3:.1f}GB")

    # 5. è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§ï¼ˆLinuxï¼‰
    if os.name == 'posix':
        print("5. ğŸ¯ è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§...")
        try:
            os.nice(-10)
            print("   âœ… è¿›ç¨‹ä¼˜å…ˆçº§å·²æå‡")
        except:
            print("   âš ï¸  æ— æ³•æå‡è¿›ç¨‹ä¼˜å…ˆçº§ï¼ˆéœ€è¦sudoæƒé™ï¼‰")

    # 6. éªŒè¯ä¼˜åŒ–ç»“æœ
    print("6. âœ… éªŒè¯ä¼˜åŒ–ç»“æœ...")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"   å¯è§GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"   cuDNNåŸºå‡†æ¨¡å¼: {torch.backends.cudnn.benchmark}")
    print(f"   TF32çŸ©é˜µä¹˜æ³•: {torch.backends.cuda.matmul.allow_tf32}")
    print(f"   TF32å·ç§¯: {torch.backends.cudnn.allow_tf32}")

    print("\n" + "=" * 60)
    print("ğŸ‰ ç³»ç»Ÿä¼˜åŒ–å®Œæˆï¼ç°åœ¨å¯ä»¥è¿è¡Œæé€Ÿè®­ç»ƒã€‚")
    print("ğŸ’¡ è¿è¡Œå‘½ä»¤: python train_30min.py")
    print("=" * 60)


def check_training_readiness():
    """æ£€æŸ¥è®­ç»ƒå‡†å¤‡çŠ¶æ€"""
    print("\nğŸ” è®­ç»ƒå‡†å¤‡çŠ¶æ€æ£€æŸ¥:")

    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
    required_files = ['train_30min.py', 'models/__init__.py', 'utils/data_loader.py']
    missing_files = []

    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)

    if missing_files:
        print(f"   âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {missing_files}")
        return False
    else:
        print("   âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å­˜åœ¨")

    # æ£€æŸ¥GPUå†…å­˜
    if torch.cuda.is_available():
        total_free_memory = 0
        for i in range(torch.cuda.device_count()):
            free_memory = torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)
            free_gb = free_memory / 1024 ** 3
            total_free_memory += free_gb

            if free_gb < 4:
                print(f"   âš ï¸  GPU {i} ç©ºé—²å†…å­˜ä¸è¶³: {free_gb:.1f}GB (å»ºè®®â‰¥4GB)")
            else:
                print(f"   âœ… GPU {i} ç©ºé—²å†…å­˜å……è¶³: {free_gb:.1f}GB")

        print(f"   ğŸ“Š æ€»å¯ç”¨GPUå†…å­˜: {total_free_memory:.1f}GB")

        # æ ¹æ®æ€»å†…å­˜æ¨èæ‰¹å¤§å°
        if total_free_memory >= 60:
            recommended_batch = 4096
        elif total_free_memory >= 40:
            recommended_batch = 2048
        elif total_free_memory >= 20:
            recommended_batch = 1024
        else:
            recommended_batch = 512

        print(f"   ğŸ’¡ æ¨èæ‰¹å¤§å°: {recommended_batch}")

    return True


if __name__ == '__main__':
    gpu_ids = [3, 4, 5, 6]
    optimize_system(gpu_ids)

    # æ£€æŸ¥è®­ç»ƒå‡†å¤‡çŠ¶æ€
    if check_training_readiness():
        print("\nğŸ¯ ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œæé€Ÿè®­ç»ƒï¼")
    else:
        print("\nâŒ è¯·å…ˆè§£å†³ä¸Šè¿°é—®é¢˜å†å¼€å§‹è®­ç»ƒã€‚")